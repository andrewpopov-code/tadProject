{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-21T11:48:19.048075Z",
     "start_time": "2024-04-21T11:48:11.950092Z"
    }
   },
   "source": [
    "from nn import IntrinsicMixin, IntrinsicObserver, IntrinsicTrainingObserver\n",
    "\n",
    "# PyTorch model and training necessities\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Image datasets and image manipulation\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Image display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# from tensorboardX import SummaryWriter\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Gather datasets and prepare them for consumption\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Store separate training and validations splits in ./data\n",
    "training_set = torchvision.datasets.FashionMNIST('./data',\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=transform)\n",
    "validation_set = torchvision.datasets.FashionMNIST('./data',\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transform)\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(training_set,\n",
    "                                              batch_size=4,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=2)\n",
    "\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set,\n",
    "                                                batch_size=4,\n",
    "                                                shuffle=False,\n",
    "                                                num_workers=2)\n",
    "\n",
    "# Class labels\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
    "\n",
    "# Helper function for inline image display\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "# Extract a batch of 4 images\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Create a grid from the images and show them\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "matplotlib_imshow(img_grid, one_channel=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T11:48:23.280199Z",
     "start_time": "2024-04-21T11:48:19.057545Z"
    }
   },
   "id": "b45163daee01868c",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Default log_dir argument is \"runs\" - but it's good to be specific\n",
    "# torch.utils.tensorboard.SummaryWriter is imported above\n",
    "writer = SummaryWriter('runs/fashion_mnist_experiment_23')\n",
    "\n",
    "# Write image data to TensorBoard log dir\n",
    "writer.add_image('Four Fashion-MNIST Images', img_grid)\n",
    "writer.flush()\n",
    "\n",
    "# To view, start TensorBoard on the command line with:\n",
    "#   tensorboard --logdir=runs\n",
    "# ...and open a browser tab to http://localhost:6006/"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T11:48:23.300800Z",
     "start_time": "2024-04-21T11:48:23.284255Z"
    }
   },
   "id": "c49af50de8128069",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class Net(nn.Module, TopologyMixin):\n",
    "    def __init__(self):\n",
    "        nn.Module.__init__(self)\n",
    "        TopologyMixin.__init__(self, writer=writer)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.Filtration(x, label='Input', logging=False)\n",
    "        self.Dimension(x, label='Dimension Analysis', distances=False, batches=True)\n",
    "        self.DeltaHyperbolicity(x, label='Hyperbolicity Analysis', distances=False, batches=True)\n",
    "        \n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        self.Filtration(x, label='Hidden', distances=False, batches=True)\n",
    "        self.Dimension(x, label='Dimension Analysis', distances=False, batches=True)\n",
    "        self.DeltaHyperbolicity(x, label='Hyperbolicity Analysis', distances=False, batches=True)\n",
    "        \n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T17:51:16.594648Z",
     "start_time": "2024-03-31T17:51:16.576276Z"
    }
   },
   "id": "3d705b89de345ab1",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "observer = TopologyTrainingObserver(net, writer=writer, reset=True, log_every_train=1000, log_every_val=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T17:51:16.596588Z",
     "start_time": "2024-03-31T17:51:16.590240Z"
    }
   },
   "id": "2e3cc2a54c6bd419",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "net.Filtration.forward"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T17:49:09.554163Z",
     "start_time": "2024-03-31T17:49:09.542246Z"
    }
   },
   "id": "d348dc1a40a9454",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "net.train(True)\n",
    "\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    print(observer.step)\n",
    "\n",
    "    for i, data in enumerate(training_loader, 0):\n",
    "        # basic training loop\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 1000 == 0:    # Every 1000 mini-batches...\n",
    "            print('Batch {}'.format(i + 1))\n",
    "            # Check against the validation set\n",
    "            running_vloss = 0.0\n",
    "\n",
    "            # In evaluation mode some model specific operations can be omitted eg. dropout layer\n",
    "            net.train(False) # Switching to evaluation mode, eg. turning off regularisation\n",
    "            \n",
    "            for j, vdata in enumerate(validation_loader, 0):\n",
    "                vinputs, vlabels = vdata\n",
    "                voutputs = net(vinputs)\n",
    "                vloss = criterion(voutputs, vlabels)\n",
    "                running_vloss += vloss.item()\n",
    "                \n",
    "            net.train(True) # Switching back to training mode, eg. turning on regularisation\n",
    "\n",
    "            avg_loss = running_loss / 10\n",
    "            avg_vloss = running_vloss / 20\n",
    "\n",
    "            # Log the running loss averaged per batch\n",
    "            writer.add_scalars('Training vs. Validation Loss',\n",
    "                            { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                            epoch * len(training_loader) + i)\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-31T17:51:18.688671Z"
    }
   },
   "id": "852d3701c687d7b7",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "observer.topology_modules"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T16:18:09.590469Z",
     "start_time": "2024-03-31T16:18:09.567103Z"
    }
   },
   "id": "270a16f055f4b880",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "observer.topology_modules_forward"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T16:18:16.862045Z",
     "start_time": "2024-03-31T16:18:16.846272Z"
    }
   },
   "id": "7003c9c368e40e8e",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        nn.Module.__init__(self)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T11:48:23.342373Z",
     "start_time": "2024-04-21T11:48:23.301351Z"
    }
   },
   "id": "abbdae16a1a5fd96",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from nn.topology import Persistence, DeltaHyperbolicity, Dimension"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T11:48:23.344284Z",
     "start_time": "2024-04-21T11:48:23.317738Z"
    }
   },
   "id": "866531af0daf5bb5",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "filtration = Persistence()\n",
    "dim = Dimension()\n",
    "dh = DeltaHyperbolicity()\n",
    "\n",
    "observer = IntrinsicTrainingObserver(\n",
    "    net, writer=writer, pre_topology=[\n",
    "        (net.conv1, [\n",
    "            (filtration, {'label': 'Input'}),\n",
    "            (dim, {'label': 'Dimension Analysis'}),\n",
    "            (dh, {'label': 'Hyperbolicity Analysis'})\n",
    "        ]),\n",
    "        (net.fc1, [\n",
    "            (filtration, {'label': 'Hidden'}, lambda x: x.reshape(-1, 16, 4, 4)),\n",
    "            (dim, {'label': 'Dimension Analysis'}, lambda x: x.reshape(-1, 16, 4, 4)),\n",
    "            (dh, {'label': 'Hyperbolicity Analysis'}, lambda x: x.reshape(-1, 16, 4, 4))\n",
    "        ])\n",
    "    ],\n",
    "    log_every_val=1000, log_every_train=1000\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T11:48:23.346578Z",
     "start_time": "2024-04-21T11:48:23.325403Z"
    }
   },
   "id": "5c46c74c33a6622a",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "Filtration = Persistence()\n",
    "Dimension = IntrinsicDimension()\n",
    "DH = DeltaHyperbolicity()\n",
    "\n",
    "observer = TopologyObserver(\n",
    "    net, writer=writer, pre_topology=[\n",
    "        (net.conv1, [\n",
    "            (Filtration, {'label': 'Input'}),\n",
    "            (Dimension, {'label': 'Dimension Analysis'}),\n",
    "            (DH, {'label': 'Hyperbolicity Analysis'})\n",
    "        ]),\n",
    "        (net.fc1, [\n",
    "            (Filtration, {'label': 'Hidden'}, lambda x: x.reshape(-1, 16, 4, 4)),\n",
    "            (Dimension, {'label': 'Dimension Analysis'}, lambda x: x.reshape(-1, 16, 4, 4)),\n",
    "            (DH, {'label': 'Hyperbolicity Analysis'}, lambda x: x.reshape(-1, 16, 4, 4))\n",
    "        ])\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T11:47:01.701720Z",
     "start_time": "2024-04-13T11:47:01.674966Z"
    }
   },
   "id": "a0acac0767f709e9",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "### Example"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T18:35:53.058345Z",
     "start_time": "2024-04-13T18:35:53.034605Z"
    }
   },
   "id": "2d5e4389d1b12393",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2d04a89a7c0371c"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Log Validation Results\n",
    "net.eval()\n",
    "vdata = next(iter(validation_loader))\n",
    "vinputs, vlabels = vdata\n",
    "\n",
    "dimension = IntrinsicDimension()\n",
    "dh = DeltaHyperbolicity()\n",
    "# voutputs = net(vinputs)\n",
    "# vloss = criterion(voutputs, vlabels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T18:36:50.672979Z",
     "start_time": "2024-04-13T18:36:43.619886Z"
    }
   },
   "id": "61e1ae88196ec53d",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "dh(vinputs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T18:45:40.435772Z",
     "start_time": "2024-04-13T18:45:40.341774Z"
    }
   },
   "id": "1b47dc1b577d1978",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "dimension(vinputs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T18:45:21.732477Z",
     "start_time": "2024-04-13T18:45:18.970225Z"
    }
   },
   "id": "e9a2bb967d7fa34d",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bb365e98e5492d8e"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from utils.math import compute_unique_distances\n",
    "\n",
    "batch = vinputs[0]\n",
    "from nn.functional.dimension import mle, mm\n",
    "\n",
    "d = compute_unique_distances(batch).detach().numpy()\n",
    "mle(d)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T18:48:25.004045Z",
     "start_time": "2024-04-13T18:48:24.953051Z"
    }
   },
   "id": "ec09c3514451933b",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "from scipy.spatial import distance_matrix\n",
    "def pairwise_dist(bc: np.array):\n",
    "    return [\n",
    "        distance_matrix(bc[:, dim], bc[:, dim]) for dim in range(len(bc[0]))\n",
    "    ]\n",
    "\n",
    "bc = np.array([dgrm.betti[0] for dgrm in Filtration.diagrams])\n",
    "pairwise_dist(bc)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T11:37:23.936062Z",
     "start_time": "2024-04-13T11:37:23.927946Z"
    }
   },
   "id": "ab95090e9ecfd050",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from nn.functional.homology import pairwise_dist\n",
    "pairwise_dist(np.array(bc))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T11:18:38.698605Z",
     "start_time": "2024-04-13T11:18:38.692338Z"
    }
   },
   "id": "b3ccb1a0422452a4",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "Filtration.diagrams"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T10:04:15.284666Z",
     "start_time": "2024-04-13T10:04:15.252628Z"
    }
   },
   "id": "2e4e4fdf7170f1b4",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "Filtration = Persistence()\n",
    "Dimension = IntrinsicDimension()\n",
    "DH = DeltaHyperbolicity()\n",
    "\n",
    "observer = TopologyTrainingObserver(\n",
    "    net, writer=writer, pre_topology=[\n",
    "        (net.conv1, [\n",
    "            (Filtration, {'label': 'Input'}),\n",
    "            (Dimension, {'label': 'Dimension Analysis'}),\n",
    "            (DH, {'label': 'Hyperbolicity Analysis'})\n",
    "        ]),\n",
    "        (net.fc1, [\n",
    "            (Filtration, {'label': 'Hidden'}, lambda x: x.reshape(-1, 16, 4, 4)),\n",
    "            (Dimension, {'label': 'Dimension Analysis'}, lambda x: x.reshape(-1, 16, 4, 4)),\n",
    "            (DH, {'label': 'Hyperbolicity Analysis'}, lambda x: x.reshape(-1, 16, 4, 4))\n",
    "        ])\n",
    "    ],\n",
    "    log_every_train=1000,\n",
    "    log_every_val=1000\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T17:32:49.812609Z",
     "start_time": "2024-04-07T17:32:49.790918Z"
    }
   },
   "id": "f8feb68913c1ead3",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "net.conv1._forward_pre_hooks"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T12:44:48.847779Z",
     "start_time": "2024-03-31T12:44:48.837892Z"
    }
   },
   "id": "39557b6babe8e0fe",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "net.train(True)\n",
    "\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(training_loader, 0):\n",
    "        # basic training loop\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 1000 == 0:    # Every 1000 mini-batches...\n",
    "            print('Batch {}'.format(i + 1))\n",
    "            # Check against the validation set\n",
    "            running_vloss = 0.0\n",
    "\n",
    "            # In evaluation mode some model specific operations can be omitted eg. dropout layer\n",
    "            net.train(False) # Switching to evaluation mode, eg. turning off regularisation\n",
    "            \n",
    "            for j, vdata in enumerate(validation_loader, 0):\n",
    "                vinputs, vlabels = vdata\n",
    "                voutputs = net(vinputs)\n",
    "                vloss = criterion(voutputs, vlabels)\n",
    "                running_vloss += vloss.item()\n",
    "                \n",
    "            net.train(True) # Switching back to training mode, eg. turning on regularisation\n",
    "\n",
    "            avg_loss = running_loss / 10\n",
    "            avg_vloss = running_vloss / 20\n",
    "\n",
    "            # Log the running loss averaged per batch\n",
    "            writer.add_scalars('Training vs. Validation Loss',\n",
    "                            { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                            epoch * len(training_loader) + i)\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T11:49:30.798870Z",
     "start_time": "2024-04-21T11:48:23.345715Z"
    }
   },
   "id": "b56d90cbedf506c7",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "ret1, ret2 = filtration.divergence(filtration.diagrams)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T11:57:28.867227Z",
     "start_time": "2024-04-21T11:57:28.683120Z"
    }
   },
   "id": "2e5930f1bdd95513",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "observer.train_epoch_information[-1][(5884511808, 'Input')][0].sample.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T12:00:44.274531Z",
     "start_time": "2024-04-21T12:00:44.258559Z"
    }
   },
   "id": "b413b60a7e5c5474",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "inputs, labels = next(iter(training_loader))\n",
    "inputs.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T11:33:45.153628Z",
     "start_time": "2024-04-21T11:33:40.401039Z"
    }
   },
   "id": "a5c6f71d4c234e5f",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "x = inputs.transpose(1, 2).transpose(2, 3).detach().numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T11:36:16.657897Z",
     "start_time": "2024-04-21T11:36:16.626008Z"
    }
   },
   "id": "9b80f437fce64d4",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "from utils.math import unique_points\n",
    "unique_points(x[0]).shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T11:36:26.385899Z",
     "start_time": "2024-04-21T11:36:26.368848Z"
    }
   },
   "id": "81c15bb50e85550c",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "observer.val_epoch_information"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T12:06:17.579563Z",
     "start_time": "2024-04-06T12:06:17.554553Z"
    }
   },
   "id": "cd661454430aa845",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "Filtration._forward_hooks"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T17:32:50.195070Z",
     "start_time": "2024-04-07T17:32:50.185756Z"
    }
   },
   "id": "30d0b881a1f4506d",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
